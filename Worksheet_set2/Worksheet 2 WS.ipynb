{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 -> Main Page\n",
      "h1 -> Welcome to Wikipedia\n",
      "h2 -> From today's featured article\n",
      "h2 -> Did you know ...\n",
      "h2 -> In the news\n",
      "h2 -> On this day\n",
      "h2 -> From today's featured list\n",
      "h2 -> Today's featured picture\n",
      "h2 -> Other areas of Wikipedia\n",
      "h2 -> Wikipedia's sister projects\n",
      "h2 -> Wikipedia languages\n",
      "h2 -> Navigation menu\n",
      "h3 -> Personal tools\n",
      "h3 -> Namespaces\n",
      "h3 -> Views\n",
      "h3 -> Search\n",
      "h3 -> Navigation\n",
      "h3 -> Contribute\n",
      "h3 -> Tools\n",
      "h3 -> Print/export\n",
      "h3 -> In other projects\n",
      "h3 -> Languages\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to display all the header tags from wikipedia.org.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "url_link = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "request = requests.get(url_link)\n",
    " \n",
    "Soup = BeautifulSoup(request.text, 'lxml')\n",
    " \n",
    "# creating a list of all common heading tags\n",
    "heading_tags = [\"h1\", \"h2\", \"h3\"]\n",
    "for tags in Soup.find_all(heading_tags):\n",
    "    print(tags.name + ' -> ' + tags.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>rating</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.The Shawshank Redemption(1994)</td>\n",
       "      <td>9.3</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.The Godfather(1972)</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1972)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.The Dark Knight(2008)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.The Lord of the Rings: The Return of the Kin...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(2003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.Schindler's List(1993)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.The Godfather Part II(1974)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(1974)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.12 Angry Men(1957)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>(1957)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.Pulp Fiction(1994)</td>\n",
       "      <td>8.9</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.Inception(2010)</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.The Lord of the Rings: The Two Towers(2002)</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.Fight Club(1999)</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.The Lord of the Rings: The Fellowship of th...</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.Forrest Gump(1994)</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.Il buono, il brutto, il cattivo(1966)</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(1966)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.The Matrix(1999)</td>\n",
       "      <td>8.7</td>\n",
       "      <td>(1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.Goodfellas(1990)</td>\n",
       "      <td>8.7</td>\n",
       "      <td>(1990)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.The Empire Strikes Back(1980)</td>\n",
       "      <td>8.7</td>\n",
       "      <td>(1980)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.One Flew Over the Cuckoo's Nest(1975)</td>\n",
       "      <td>8.7</td>\n",
       "      <td>(1975)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.Top Gun: Maverick(2022)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(2022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.Interstellar(2014)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.Cidade de Deus(2002)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.Sen to Chihiro no kamikakushi(2001)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.Saving Private Ryan(1998)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.The Green Mile(1999)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.La vita è bella(1997)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.Se7en(1995)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.Terminator 2: Judgment Day(1991)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.The Silence of the Lambs(1991)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.Star Wars(1977)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1977)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.Seppuku(1962)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1962)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31.Shichinin no samurai(1954)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1954)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32.It's a Wonderful Life(1946)</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1946)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33.Gisaengchung(2019)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34.Whiplash(2014)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35.The Intouchables(2011)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36.The Prestige(2006)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37.The Departed(2006)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38.The Pianist(2002)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39.Gladiator(2000)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40.American History X(1998)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41.The Usual Suspects(1995)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42.Léon(1994)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43.The Lion King(1994)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44.Nuovo Cinema Paradiso(1988)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1988)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45.Hotaru no haka(1988)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1988)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46.Back to the Future(1985)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1985)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47.Apocalypse Now(1979)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1979)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48.Alien(1979)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1979)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49.Once Upon a Time in the West(1968)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1968)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50.Psycho(1960)</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1960)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           movie_name rating    year\n",
       "0                    1.The Shawshank Redemption(1994)    9.3  (1994)\n",
       "1                               2.The Godfather(1972)    9.2  (1972)\n",
       "2                             3.The Dark Knight(2008)    9.0  (2008)\n",
       "3   4.The Lord of the Rings: The Return of the Kin...    9.0  (2003)\n",
       "4                            5.Schindler's List(1993)    9.0  (1993)\n",
       "5                       6.The Godfather Part II(1974)    9.0  (1974)\n",
       "6                                7.12 Angry Men(1957)    9.0  (1957)\n",
       "7                                8.Pulp Fiction(1994)    8.9  (1994)\n",
       "8                                   9.Inception(2010)    8.8  (2010)\n",
       "9      10.The Lord of the Rings: The Two Towers(2002)    8.8  (2002)\n",
       "10                                11.Fight Club(1999)    8.8  (1999)\n",
       "11  12.The Lord of the Rings: The Fellowship of th...    8.8  (2001)\n",
       "12                              13.Forrest Gump(1994)    8.8  (1994)\n",
       "13           14.Il buono, il brutto, il cattivo(1966)    8.8  (1966)\n",
       "14                                15.The Matrix(1999)    8.7  (1999)\n",
       "15                                16.Goodfellas(1990)    8.7  (1990)\n",
       "16                   17.The Empire Strikes Back(1980)    8.7  (1980)\n",
       "17           18.One Flew Over the Cuckoo's Nest(1975)    8.7  (1975)\n",
       "18                         19.Top Gun: Maverick(2022)    8.6  (2022)\n",
       "19                              20.Interstellar(2014)    8.6  (2014)\n",
       "20                            21.Cidade de Deus(2002)    8.6  (2002)\n",
       "21             22.Sen to Chihiro no kamikakushi(2001)    8.6  (2001)\n",
       "22                       23.Saving Private Ryan(1998)    8.6  (1998)\n",
       "23                            24.The Green Mile(1999)    8.6  (1999)\n",
       "24                           25.La vita è bella(1997)    8.6  (1997)\n",
       "25                                     26.Se7en(1995)    8.6  (1995)\n",
       "26                27.Terminator 2: Judgment Day(1991)    8.6  (1991)\n",
       "27                  28.The Silence of the Lambs(1991)    8.6  (1991)\n",
       "28                                 29.Star Wars(1977)    8.6  (1977)\n",
       "29                                   30.Seppuku(1962)    8.6  (1962)\n",
       "30                      31.Shichinin no samurai(1954)    8.6  (1954)\n",
       "31                     32.It's a Wonderful Life(1946)    8.6  (1946)\n",
       "32                              33.Gisaengchung(2019)    8.5  (2019)\n",
       "33                                  34.Whiplash(2014)    8.5  (2014)\n",
       "34                          35.The Intouchables(2011)    8.5  (2011)\n",
       "35                              36.The Prestige(2006)    8.5  (2006)\n",
       "36                              37.The Departed(2006)    8.5  (2006)\n",
       "37                               38.The Pianist(2002)    8.5  (2002)\n",
       "38                                 39.Gladiator(2000)    8.5  (2000)\n",
       "39                        40.American History X(1998)    8.5  (1998)\n",
       "40                        41.The Usual Suspects(1995)    8.5  (1995)\n",
       "41                                      42.Léon(1994)    8.5  (1994)\n",
       "42                             43.The Lion King(1994)    8.5  (1994)\n",
       "43                     44.Nuovo Cinema Paradiso(1988)    8.5  (1988)\n",
       "44                            45.Hotaru no haka(1988)    8.5  (1988)\n",
       "45                        46.Back to the Future(1985)    8.5  (1985)\n",
       "46                            47.Apocalypse Now(1979)    8.5  (1979)\n",
       "47                                     48.Alien(1979)    8.5  (1979)\n",
       "48              49.Once Upon a Time in the West(1968)    8.5  (1968)\n",
       "49                                    50.Psycho(1960)    8.5  (1960)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)and make data frame.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page = requests.get('https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc')\n",
    "soup = BeautifulSoup(page.content)\n",
    "movie_name =[]\n",
    "for  i in soup.find_all('h3', class_=\"lister-item-header\"):\n",
    "     movie_name.append(i.text.replace('\\n',''))\n",
    "movie_name\n",
    "rating =[]\n",
    "for  i in soup.find_all('div', class_=\"inline-block ratings-imdb-rating\"):\n",
    "     rating.append(i.text.replace('\\n',''))\n",
    "rating\n",
    "year =[]\n",
    "for  i in soup.find_all('span', class_=\"lister-item-year text-muted unbold\"):\n",
    "     year.append(i.text)\n",
    "year\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'movie_name':movie_name,'rating':rating,'year':year})\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>rating</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.Rang De Basanti(2006)</td>\n",
       "      <td>8.1</td>\n",
       "      <td>(2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.3 Idiots(2009)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.Taare Zameen Par(2007)</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(2007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.Dil Chahta Hai(2001)</td>\n",
       "      <td>8.1</td>\n",
       "      <td>(2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.Swades: We, the People(2004)</td>\n",
       "      <td>8.1</td>\n",
       "      <td>(2004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.Wake Up Sid(2009)</td>\n",
       "      <td>7.6</td>\n",
       "      <td>(2009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.Rangeela(1995)</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.Shatranj Ke Khilari(1977)</td>\n",
       "      <td>7.5</td>\n",
       "      <td>(1977)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.Pyaar Ka Punchnama(2011)</td>\n",
       "      <td>7.6</td>\n",
       "      <td>(2011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.Ek Hasina Thi(2004)</td>\n",
       "      <td>7.5</td>\n",
       "      <td>(2004)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        movie_name rating    year\n",
       "0          1.Rang De Basanti(2006)    8.1  (2006)\n",
       "1                 2.3 Idiots(2009)    8.4  (2009)\n",
       "2         3.Taare Zameen Par(2007)    8.3  (2007)\n",
       "3           4.Dil Chahta Hai(2001)    8.1  (2001)\n",
       "4   5.Swades: We, the People(2004)    8.1  (2004)\n",
       "..                             ...    ...     ...\n",
       "95            96.Wake Up Sid(2009)    7.6  (2009)\n",
       "96               97.Rangeela(1995)    7.4  (1995)\n",
       "97    98.Shatranj Ke Khilari(1977)    7.5  (1977)\n",
       "98     99.Pyaar Ka Punchnama(2011)    7.6  (2011)\n",
       "99         100.Ek Hasina Thi(2004)    7.5  (2004)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page = requests.get('https://www.imdb.com/list/ls009997493/')\n",
    "soup = BeautifulSoup(page.content)\n",
    "movie_name =[]\n",
    "for  i in soup.find_all('h3', class_=\"lister-item-header\"):\n",
    "     movie_name.append(i.text.replace('\\n',''))\n",
    "movie_name\n",
    "rating =[]\n",
    "for  i in soup.find_all('div', class_=\"ipl-rating-star small\"):\n",
    "     rating.append(i.text.replace('\\n',''))\n",
    "rating\n",
    "year =[]\n",
    "for  i in soup.find_all('span', class_=\"lister-item-year text-muted unbold\"):\n",
    "     year.append(i.text)\n",
    "year\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'movie_name':movie_name,'rating':rating,'year':year})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind (birth - 1945)Term of Off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee (1935-2020)Term of Offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil (birth - 1934)Ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam (1931-2015)Term of Offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan (1920 - 2005)Term of Offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma (1918-1999)Term of Off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman (1910-2009)Term of Office:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh (1916-1994)Term of Office: 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy (1913-1996)Term of O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed (1905-1977)Term of Of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri (1894-1980)Term o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain (1897-1969)Term of Office: 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan (1888-1975)Term o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad (1884-1963) Term of Office...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name\n",
       "0   Shri Ram Nath Kovind (birth - 1945)Term of Off...\n",
       "1   Shri Pranab Mukherjee (1935-2020)Term of Offic...\n",
       "2   Smt Pratibha Devisingh Patil (birth - 1934)Ter...\n",
       "3   DR. A.P.J. Abdul Kalam (1931-2015)Term of Offi...\n",
       "4   Shri K. R. Narayanan (1920 - 2005)Term of Offi...\n",
       "5   Dr Shankar Dayal Sharma (1918-1999)Term of Off...\n",
       "6   Shri R Venkataraman (1910-2009)Term of Office:...\n",
       "7   Giani Zail Singh (1916-1994)Term of Office: 25...\n",
       "8   Shri Neelam Sanjiva Reddy (1913-1996)Term of O...\n",
       "9   Dr. Fakhruddin Ali Ahmed (1905-1977)Term of Of...\n",
       "10  Shri Varahagiri Venkata Giri (1894-1980)Term o...\n",
       "11  Dr. Zakir Husain (1897-1969)Term of Office: 13...\n",
       "12  Dr. Sarvepalli Radhakrishnan (1888-1975)Term o...\n",
       "13  Dr. Rajendra Prasad (1884-1963) Term of Office..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write s python program to display list of respected former presidents of India\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page = requests.get('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "soup = BeautifulSoup(page.content)\n",
    "name =[]\n",
    "for  i in soup.find_all('div', class_=\"presidentListing\"):\n",
    "     name.append(i.text.replace('\\n',''))\n",
    "name\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'name':name})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>3,226</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>3,085</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>2,005</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>2,325</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>2,111</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27</td>\n",
       "      <td>2,639</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29</td>\n",
       "      <td>2,658</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38</td>\n",
       "      <td>2,621</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18</td>\n",
       "      <td>1,238</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23</td>\n",
       "      <td>1,214</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Matches Points Ratings\n",
       "0      27  3,226     119\n",
       "1      28  3,085     110\n",
       "2      19  2,005     106\n",
       "3      23  2,325     101\n",
       "4      21  2,111     101\n",
       "5      27  2,639      98\n",
       "6      29  2,658      92\n",
       "7      38  2,621      69\n",
       "8      18  1,238      69\n",
       "9      23  1,214      53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "ODI_teams=BeautifulSoup(requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi').content,'html.parser')\n",
    "team=[ i.text for i in ODI_teams.find_all('span',class_='u-hide-phablet')]\n",
    "matches=[ i.text for i in ODI_teams.find_all('td',class_='table-body__cell u-center-text')]\n",
    "matches1=[matches[i] for i in range(0,len(matches),2)][:10]\n",
    "points=[matches[i] for i in range(1,len(matches),2)][:10]\n",
    "ratings=[ i.text for i in ODI_teams.find_all('td',class_='table-body__cell u-text-right rating')][:10]\n",
    "import pandas as pd\n",
    "df=pd.DataFrame({'Matches':matches1,'Points':points,'Ratings':ratings})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batsmen_Names</th>\n",
       "      <th>Team_Names</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aaron Finch</td>\n",
       "      <td>AUS</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Batsmen_Names Team_Names Rating\n",
       "0               Babar Azam        PAK    815\n",
       "1             Imam-ul-Haq         PAK    815\n",
       "2   Rassie van der Dussen          SA    789\n",
       "3         Quinton de Kock          SA    784\n",
       "4             Virat Kohli         IND    767\n",
       "5            Rohit Sharma         IND    763\n",
       "6             Ross Taylor          NZ    744\n",
       "7            David Warner         AUS    737\n",
       "8          Jonny Bairstow         ENG    732\n",
       "9             Aaron Finch         AUS    715"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import requests as rs\n",
    "page_ODI_Bat = rs.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi\")\n",
    "ODI_soup_Bat = Soup(page_ODI_Bat.content , \"html.parser\")\n",
    "Batsmen_Name = []\n",
    "Team = []\n",
    "Rating =  []\n",
    "Batsmen = ODI_soup_Bat.find_all(\"td\", class_ =\"table-body__cell name\")\n",
    "Batsmen[0:4]\n",
    "for i in Batsmen:\n",
    "    Batsmen_Name.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "    \n",
    "Batsmen_Name[0:3]\n",
    "Team_Name = ODI_soup_Bat.find_all(\"span\", class_=\"table-body__logo-text\")\n",
    "Team_Name[0:4]\n",
    "for i in Team_Name:\n",
    "    Team.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Team[0:4]\n",
    "Rating_Bat = ODI_soup_Bat.find_all(\"td\", class_=\"table-body__cell u-text-right rating\")\n",
    "Rating_Bat[0:4]\n",
    "for i in Rating_Bat:\n",
    "    Rating.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Rating[0:4]\n",
    "Batsmen_Name.insert(0 , 'Babar Azam')\n",
    "Team.insert(0 ,'PAK')\n",
    "Rating.insert(0,'815')\n",
    "import pandas as pd\n",
    "\n",
    "Top_Batsmen_ODI = pd.DataFrame({})\n",
    "Top_Batsmen_ODI[\"Batsmen_Names\"]=Batsmen_Name\n",
    "Top_Batsmen_ODI[\"Team_Names\"]=Team\n",
    "Top_Batsmen_ODI[\"Rating\"]=Rating\n",
    "Top_Batsmen_ODI.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batsmen_Names</th>\n",
       "      <th>Team_Names</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mohammad Nabi</td>\n",
       "      <td>AFG</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chris Woakes</td>\n",
       "      <td>ENG</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Batsmen_Names Team_Names Rating\n",
       "0         Trent Boult         NZ    697\n",
       "1     Jasprit Bumrah         IND    682\n",
       "2     Shaheen Afridi         PAK    681\n",
       "3     Josh Hazlewood         AUS    679\n",
       "4   Mujeeb Ur Rahman         AFG    676\n",
       "5       Mehedi Hasan         BAN    672\n",
       "6         Matt Henry          NZ    663\n",
       "7      Mohammad Nabi         AFG    657\n",
       "8        Rashid Khan         AFG    651\n",
       "9       Chris Woakes         ENG    640"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import requests as rs\n",
    "page_ODI_Bow = rs.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")\n",
    "ODI_soup_Bow = Soup(page_ODI_Bow.content , \"html.parser\")\n",
    "Bowler_Name = []\n",
    "Teams = []\n",
    "Ratings =  []\n",
    "Bowler = ODI_soup_Bow.find_all(\"td\", class_ =\"table-body__cell rankings-table__name name\")\n",
    "Bowler[0:4]\n",
    "for i in Bowler:\n",
    "    Bowler_Name.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "    \n",
    "Bowler_Name[0:3]\n",
    "Team_Name_B = ODI_soup_Bow.find_all(\"span\", class_=\"table-body__logo-text\")\n",
    "Team_Name_B[0:4]\n",
    "for i in Team_Name_B:\n",
    "    Teams.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Teams[0:4]\n",
    "Rating_Bows = ODI_soup_Bow.find_all(\"td\", class_=\"table-body__cell rating\")\n",
    "Rating_Bows[0:4]\n",
    "for i in Rating_Bows:\n",
    "    Ratings.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Ratings[0:4]\n",
    "Bowler_Name.insert(0 , 'Trent Boult')\n",
    "Teams.insert(0 ,'NZ')\n",
    "Ratings.insert(0,'697')\n",
    "import pandas as pd\n",
    "\n",
    "Top_Bowler_ODI = pd.DataFrame({})\n",
    "Top_Bowler_ODI[\"Batsmen_Names\"]=Bowler_Name\n",
    "Top_Bowler_ODI[\"Team_Names\"]=Teams\n",
    "Top_Bowler_ODI[\"Rating\"]=Ratings\n",
    "\n",
    "Top_Bowler_ODI.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['33', '35', '32', '31', '30', '12', '30', '11', '8', '8']\n",
      "['4,046', '4,157', '3,219', '3,019', '2,768', '930', '1,962', '495', '351', '0']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team_name</th>\n",
       "      <th>Total_Match</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>33</td>\n",
       "      <td>4,046</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>35</td>\n",
       "      <td>4,157</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>32</td>\n",
       "      <td>3,219</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>31</td>\n",
       "      <td>3,019</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>30</td>\n",
       "      <td>2,768</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>12</td>\n",
       "      <td>930</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30</td>\n",
       "      <td>1,962</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>11</td>\n",
       "      <td>495</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>8</td>\n",
       "      <td>351</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Team_name Total_Match Points Rating\n",
       "0     Australia          33  4,046    123\n",
       "1       England          35  4,157    119\n",
       "2  South Africa          32  3,219    101\n",
       "3         India          31  3,019     97\n",
       "4   New Zealand          30  2,768     92\n",
       "5   West Indies          12    930     78\n",
       "6    Bangladesh          30  1,962     65\n",
       "7      Pakistan          11    495     45\n",
       "8     Sri Lanka           8    351     44\n",
       "9       Ireland           8      0      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import requests as rs\n",
    "page_ODI_W = rs.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")\n",
    "ODI_soup_W = Soup(page_ODI_W.content , \"html.parser\")\n",
    "Team = []\n",
    "Match = []\n",
    "Points = []\n",
    "Rating =  []\n",
    "W_Team = ODI_soup_W.find_all(\"span\", class_=\"u-hide-phablet\")\n",
    "W_Team[0:4]\n",
    "for i in W_Team:\n",
    "    Team.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "Team=Team[0:10]\n",
    "Team[0:11]\n",
    "ODI_Match_W = ODI_soup_W.find_all(\"td\", class_=\"table-body__cell u-center-text\")\n",
    "ODI_Match_W[0:4]\n",
    "for i in ODI_Match_W:\n",
    "    Match.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Match[0:4]\n",
    "Total_Match=[]\n",
    "i=0\n",
    "while i < len(Match):\n",
    "    Total_Match.append(Match[i])\n",
    "    i += 2\n",
    "        \n",
    "print(Total_Match)\n",
    "Points=[]\n",
    "i=1\n",
    "while i < len(Match):\n",
    "    Points.append(Match[i])\n",
    "    i += 2\n",
    "        \n",
    "print(Points) \n",
    "ODI_Rating_W = ODI_soup_W.find_all(\"td\", class_=\"table-body__cell u-text-right rating\")\n",
    "ODI_Rating_W[0:4]\n",
    "for i in ODI_Rating_W:\n",
    "    Rating.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Rating[0:4] \n",
    "import pandas as pd\n",
    "\n",
    "Top_10_Woman = pd.DataFrame({})\n",
    "Top_10_Woman[\"Team_name\"] = Team\n",
    "Top_10_Woman[\"Total_Match\"] = Total_Match\n",
    "Top_10_Woman[\"Points\"] = Points\n",
    "Top_10_Woman[\"Rating\"] = Rating\n",
    "Top_10_Woman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28 28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batsmen_Names</th>\n",
       "      <th>Team_Names</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Batsmen_Names Team_Names Rating\n",
       "0           Alyssa Healy        AUS    785\n",
       "1           Beth Mooney         AUS    749\n",
       "2        Natalie Sciver         ENG    747\n",
       "3       Laura Wolvaardt          SA    732\n",
       "4           Meg Lanning         AUS    710\n",
       "5        Rachael Haynes         AUS    701\n",
       "6     Amy Satterthwaite          NZ    681\n",
       "7        Tammy Beaumont         ENG    667\n",
       "8   Chamari Athapaththu          SL    655\n",
       "9       Smriti Mandhana         IND    649"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import requests as rs\n",
    "page_ODI_Bat_w = rs.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi\")\n",
    "ODI_soup_Bat_w = Soup(page_ODI_Bat_w.content , \"html.parser\")\n",
    "Player_Name = []\n",
    "Team_W = []\n",
    "Ratings_W =  []\n",
    "player_W = ODI_soup_Bat_w.find_all(\"td\", class_ =\"table-body__cell name\")\n",
    "player_W[0:4]\n",
    "\n",
    "for i in player_W:\n",
    "    Player_Name.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "    \n",
    "Player_Name[0:3]\n",
    "Team_Name_W = ODI_soup_Bat_w.find_all(\"span\", class_=\"table-body__logo-text\")\n",
    "Team_Name_W[0:4]\n",
    " \n",
    "for i in Team_Name_W:\n",
    "    Team_W.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Team_W[0:4]\n",
    "Rating_Bat_w = ODI_soup_Bat_w.find_all(\"td\", class_=\"table-body__cell u-text-right rating\")\n",
    "Rating_Bat_w[0:4]\n",
    "\n",
    "for i in Rating_Bat_w:\n",
    "    Ratings_W.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Ratings_W[0:4]\n",
    "Player_Name.insert(0 , 'Alyssa Healy')\n",
    "Team_W.insert(0 ,'AUS')\n",
    "Ratings_W.insert(0,'785')\n",
    "print(len(Team_W), len(Player_Name),len(Ratings_W))\n",
    "import pandas as pd\n",
    "\n",
    "Top_player_ODI_W = pd.DataFrame({})\n",
    "Top_player_ODI_W[\"Batsmen_Names\"]=Player_Name\n",
    "Top_player_ODI_W[\"Team_Names\"]=Team_W\n",
    "Top_player_ODI_W[\"Rating\"]=Ratings_W\n",
    "\n",
    "Top_player_ODI_W.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All_Rounder_Name</th>\n",
       "      <th>Team_Names</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stafanie Taylor</td>\n",
       "      <td>WI</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     All_Rounder_Name Team_Names Rating\n",
       "0      Natalie Sciver        ENG    379\n",
       "1       Ellyse Perry         AUS    374\n",
       "2     Marizanne Kapp          SA    349\n",
       "3    Hayley Matthews          WI    339\n",
       "4        Amelia Kerr          NZ    336\n",
       "5   Ashleigh Gardner         AUS    270\n",
       "6      Deepti Sharma         IND    252\n",
       "7      Jess Jonassen         AUS    246\n",
       "8    Katherine Brunt         ENG    220\n",
       "9    Stafanie Taylor          WI    207"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import requests as rs\n",
    "page_ODI_ALL = rs.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")\n",
    "ODI_soup_ALL = Soup(page_ODI_ALL.content , \"html.parser\")\n",
    "All_Rounder_Name = []\n",
    "Team_All = []\n",
    "Rating_All =  []\n",
    "All_Rounder = ODI_soup_ALL.find_all(\"td\", class_ =\"table-body__cell rankings-table__name name\")\n",
    "All_Rounder[0:4]\n",
    "\n",
    "for i in All_Rounder:\n",
    "    All_Rounder_Name.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "    \n",
    "All_Rounder_Name[0:3]\n",
    "Team_Name_All = ODI_soup_ALL.find_all(\"span\", class_=\"table-body__logo-text\")\n",
    "Team_Name_All[0:4]\n",
    " \n",
    "for i in Team_Name_All:\n",
    "    Team_All.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Team_All[0:4]\n",
    "Team_Name_All = ODI_soup_ALL.find_all(\"span\", class_=\"table-body__logo-text\")\n",
    "Team_Name_All[0:4]\n",
    " \n",
    "for i in Team_Name_All:\n",
    "    Team_All.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Team_All[0:4]\n",
    "Rating_All_W = ODI_soup_ALL.find_all(\"td\", class_=\"table-body__cell rating\")\n",
    "Rating_All_W[0:4]\n",
    " \n",
    "for i in Rating_All_W:\n",
    "    Rating_All.append(i.get_text().replace(\"\\n\",\" \"))\n",
    "\n",
    "Rating_All[0:4]\n",
    "All_Rounder_Name.insert(0 , 'Natalie Sciver')\n",
    "Team_All.insert(0 ,'ENG')\n",
    "Rating_All.insert(0,'379')\n",
    "import pandas as pd\n",
    "\n",
    "All_ODI_W = pd.DataFrame({})\n",
    "All_ODI_W[\"All_Rounder_Name\"]=All_Rounder_Name\n",
    "All_ODI_W[\"Team_Names\"]=Team_All[0:20]\n",
    "All_ODI_W[\"Rating\"]=Rating_All\n",
    "\n",
    "All_ODI_W.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>time</th>\n",
       "      <th>newslink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18 Min AgoMonday's biggest analyst calls: Tesl...</td>\n",
       "      <td>18 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/mondays-street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46 Min AgoGoldman Sachs says buy this biopharm...</td>\n",
       "      <td>46 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/goldman-sachs-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53 Min AgoDeadlock over Nord Stream gas turbin...</td>\n",
       "      <td>53 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/russia-gas-sie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Hour AgoStocks making the biggest moves in t...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/stocks-making-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 Hour AgoCash withdrawals in the UK soar as B...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/uk-cash-withdr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2 Hours AgoPalantir shares fall more than 14% ...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/palantir-pltr-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2 Hours Ago5 things to know before the stock m...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/5-things-to-kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2 Hours AgoJPMorgan upgrades First Solar, says...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/jpmorgan-says-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2 Hours AgoRoku downgraded to sell by Pivotal,...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/roku-downgrade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3 Hours AgoThe market's biggest winners and lo...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/the-market-win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3 Hours AgoJPMorgan downgrades shares of Carva...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/jpmorgan-downg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4 Hours AgoUN chief calls latest Russian attac...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/russia-ukraine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4 Hours AgoU.S. Treasury yields fall as invest...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/us-bonds-treas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6 Hours AgoSoftBank posts a $21.6 billion quar...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/softbank-visio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7 Hours AgoEuropean stocks climb as traders as...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/europe-markets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10 Hours AgoHong Kong cuts hotel quarantine fo...</td>\n",
       "      <td>10 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/hong-kong-redu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11 Hours AgoBaidu's robotaxis don't need any h...</td>\n",
       "      <td>11 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/baidus-robotax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>12 Hours AgoCramer: The Inflation Reduction Ac...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/07/cramer-the-inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12 Hours AgoWall Street likes growth stocks ag...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/wall-street-ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12 Hours AgoBlackRock: The era of steady growt...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/blackrock-era-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12 Hours AgoVideo game giants had a miserable ...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/microsoft-xbox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13 Hours AgoTech stocks drag Hong Kong's Hang ...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/08/asia-markets-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14 Hours AgoCelsius withdraws motion to hire C...</td>\n",
       "      <td>14 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/07/celsius-withdr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14 Hours AgoPotential curb on Australian LNG e...</td>\n",
       "      <td>14 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/07/potential-curb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15 Hours AgoStock futures rise following S&amp;P 5...</td>\n",
       "      <td>15 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/07/stock-market-n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16 Hours AgoClimate groups react to Senate pas...</td>\n",
       "      <td>16 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/07/climate-groups...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21 Hours AgoThe best and worst places to live ...</td>\n",
       "      <td>21 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/07/global-liveabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23 Hours AgoGeorge Clooney had tequila, Ryan R...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/07/george-clooney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>23 Hours Ago9 corporate leaders share their be...</td>\n",
       "      <td>23 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/07/9-busy-people-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24 Hours AgoBethenny Frankel: The most success...</td>\n",
       "      <td>24 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/08/07/bethenny-frank...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline          time  \\\n",
       "0   18 Min AgoMonday's biggest analyst calls: Tesl...    18 Min Ago   \n",
       "1   46 Min AgoGoldman Sachs says buy this biopharm...    46 Min Ago   \n",
       "2   53 Min AgoDeadlock over Nord Stream gas turbin...    53 Min Ago   \n",
       "3   1 Hour AgoStocks making the biggest moves in t...    1 Hour Ago   \n",
       "4   1 Hour AgoCash withdrawals in the UK soar as B...    1 Hour Ago   \n",
       "5   2 Hours AgoPalantir shares fall more than 14% ...   2 Hours Ago   \n",
       "6   2 Hours Ago5 things to know before the stock m...   2 Hours Ago   \n",
       "7   2 Hours AgoJPMorgan upgrades First Solar, says...   2 Hours Ago   \n",
       "8   2 Hours AgoRoku downgraded to sell by Pivotal,...   2 Hours Ago   \n",
       "9   3 Hours AgoThe market's biggest winners and lo...   3 Hours Ago   \n",
       "10  3 Hours AgoJPMorgan downgrades shares of Carva...   3 Hours Ago   \n",
       "11  4 Hours AgoUN chief calls latest Russian attac...   4 Hours Ago   \n",
       "12  4 Hours AgoU.S. Treasury yields fall as invest...   4 Hours Ago   \n",
       "13  6 Hours AgoSoftBank posts a $21.6 billion quar...   6 Hours Ago   \n",
       "14  7 Hours AgoEuropean stocks climb as traders as...   7 Hours Ago   \n",
       "15  10 Hours AgoHong Kong cuts hotel quarantine fo...  10 Hours Ago   \n",
       "16  11 Hours AgoBaidu's robotaxis don't need any h...  11 Hours Ago   \n",
       "17  12 Hours AgoCramer: The Inflation Reduction Ac...  12 Hours Ago   \n",
       "18  12 Hours AgoWall Street likes growth stocks ag...  12 Hours Ago   \n",
       "19  12 Hours AgoBlackRock: The era of steady growt...  12 Hours Ago   \n",
       "20  12 Hours AgoVideo game giants had a miserable ...  12 Hours Ago   \n",
       "21  13 Hours AgoTech stocks drag Hong Kong's Hang ...  13 Hours Ago   \n",
       "22  14 Hours AgoCelsius withdraws motion to hire C...  14 Hours Ago   \n",
       "23  14 Hours AgoPotential curb on Australian LNG e...  14 Hours Ago   \n",
       "24  15 Hours AgoStock futures rise following S&P 5...  15 Hours Ago   \n",
       "25  16 Hours AgoClimate groups react to Senate pas...  16 Hours Ago   \n",
       "26  21 Hours AgoThe best and worst places to live ...  21 Hours Ago   \n",
       "27  23 Hours AgoGeorge Clooney had tequila, Ryan R...  23 Hours Ago   \n",
       "28  23 Hours Ago9 corporate leaders share their be...  23 Hours Ago   \n",
       "29  24 Hours AgoBethenny Frankel: The most success...  24 Hours Ago   \n",
       "\n",
       "                                             newslink  \n",
       "0   https://www.cnbc.com/2022/08/08/mondays-street...  \n",
       "1   https://www.cnbc.com/2022/08/08/goldman-sachs-...  \n",
       "2   https://www.cnbc.com/2022/08/08/russia-gas-sie...  \n",
       "3   https://www.cnbc.com/2022/08/08/stocks-making-...  \n",
       "4   https://www.cnbc.com/2022/08/08/uk-cash-withdr...  \n",
       "5   https://www.cnbc.com/2022/08/08/palantir-pltr-...  \n",
       "6   https://www.cnbc.com/2022/08/08/5-things-to-kn...  \n",
       "7   https://www.cnbc.com/2022/08/08/jpmorgan-says-...  \n",
       "8   https://www.cnbc.com/2022/08/08/roku-downgrade...  \n",
       "9   https://www.cnbc.com/2022/08/08/the-market-win...  \n",
       "10  https://www.cnbc.com/2022/08/08/jpmorgan-downg...  \n",
       "11  https://www.cnbc.com/2022/08/08/russia-ukraine...  \n",
       "12  https://www.cnbc.com/2022/08/08/us-bonds-treas...  \n",
       "13  https://www.cnbc.com/2022/08/08/softbank-visio...  \n",
       "14  https://www.cnbc.com/2022/08/08/europe-markets...  \n",
       "15  https://www.cnbc.com/2022/08/08/hong-kong-redu...  \n",
       "16  https://www.cnbc.com/2022/08/08/baidus-robotax...  \n",
       "17  https://www.cnbc.com/2022/08/07/cramer-the-inf...  \n",
       "18  https://www.cnbc.com/2022/08/08/wall-street-ba...  \n",
       "19  https://www.cnbc.com/2022/08/08/blackrock-era-...  \n",
       "20  https://www.cnbc.com/2022/08/08/microsoft-xbox...  \n",
       "21  https://www.cnbc.com/2022/08/08/asia-markets-c...  \n",
       "22  https://www.cnbc.com/2022/08/07/celsius-withdr...  \n",
       "23  https://www.cnbc.com/2022/08/07/potential-curb...  \n",
       "24  https://www.cnbc.com/2022/08/07/stock-market-n...  \n",
       "25  https://www.cnbc.com/2022/08/07/climate-groups...  \n",
       "26  https://www.cnbc.com/2022/08/07/global-liveabi...  \n",
       "27  https://www.cnbc.com/2022/08/07/george-clooney...  \n",
       "28  https://www.cnbc.com/2022/08/07/9-busy-people-...  \n",
       "29  https://www.cnbc.com/2022/08/07/bethenny-frank...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page = requests.get('https://www.cnbc.com/world/?region=world')\n",
    "soup = BeautifulSoup(page.content)\n",
    "headline = soup.find('div', class_=\"LatestNews-headlineWrapper\")\n",
    "headline.text\n",
    "headline = []\n",
    "\n",
    "for i in soup.find_all('div', class_=\"LatestNews-headlineWrapper\"):\n",
    "    headline.append(i.text)\n",
    "    \n",
    "headline\n",
    "time = soup.find('span', class_=\"LatestNews-wrapper\")\n",
    "time.text\n",
    "time = []\n",
    "\n",
    "for i in soup.find_all('span', class_=\"LatestNews-wrapper\"):\n",
    "    time.append(i.text)\n",
    "    \n",
    "time\n",
    "newslink = []\n",
    "for i in soup.find_all(\"a\", class_=\"LatestNews-headline\"):\n",
    "    newslink.append(i['href'])\n",
    "newslink\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'headline':headline,'time':time,'newslink':newslink})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Silver, David, Singh, Satinder, Precup, Doina, Sutton, Richard S. ',\n",
       " 'Evans, Richard, Bošnjak, Matko and 5 more',\n",
       " 'Prakken, Henry, Sartor, Giovanni ',\n",
       " 'Boden, Margaret A. ',\n",
       " 'Lemaignan, Séverin, Warnier, Mathieu and 3 more',\n",
       " 'Miller, Tim ',\n",
       " 'Evans, Richard, Hernández-Orallo, José and 3 more',\n",
       " 'Sharon, Guni, Stern, Roni, Felner, Ariel, Sturtevant, Nathan R. ',\n",
       " 'Sutton, Richard S., Precup, Doina, Singh, Satinder ',\n",
       " 'Bard, Nolan, Foerster, Jakob N. and 13 more',\n",
       " 'van der Waa, Jasper, Nieuwburg, Elisabeth, Cremers, Anita, Neerincx, Mark ',\n",
       " 'Bench-Capon, T.J.M., Dunne, Paul E. ',\n",
       " 'Bošanský, Branislav, Lisý, Viliam and 3 more',\n",
       " 'Luo, Wenhan, Xing, Junliang and 4 more',\n",
       " 'Blum, Avrim L., Langley, Pat ',\n",
       " 'Arora, Saurabh, Doshi, Prashant ',\n",
       " 'Aas, Kjersti, Jullum, Martin, Løland, Anders ',\n",
       " 'Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Johannes ',\n",
       " 'Pereira, Gonçalo, Prada, Rui, Santos, Pedro A. ',\n",
       " 'Riveiro, Maria, Thill, Serge ',\n",
       " 'Kenny, Eoin M., Ford, Courtney, Quinn, Molly, Keane, Mark T. ',\n",
       " 'Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyton-Brown, Kevin ',\n",
       " 'Kohavi, Ron, John, George H. ',\n",
       " 'Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srikrishna ',\n",
       " 'Ying, Mingsheng ']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page = requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "soup = BeautifulSoup(page.content)\n",
    "paper_title = soup.find('a', class_=\"sc-5smygv-0 nrDZj\")\n",
    "paper_title.text\n",
    "\n",
    "author_name = soup.find('span', class_=\"sc-1w3fpd7-0 pgLAT\")\n",
    "author_name.text\n",
    "\n",
    "author = []\n",
    "for i in soup.find_all('span', class_=\"sc-1w3fpd7-0 pgLAT\"):\n",
    "    author.append(i.text)    \n",
    "author\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['October 2021',\n",
       " 'October 2021',\n",
       " 'October 2015',\n",
       " 'August 1998',\n",
       " 'June 2017',\n",
       " 'February 2019',\n",
       " 'April 2021',\n",
       " 'February 2015',\n",
       " 'August 1999',\n",
       " 'March 2020',\n",
       " 'February 2021',\n",
       " 'October 2007',\n",
       " 'August 2016',\n",
       " 'April 2021',\n",
       " 'December 1997',\n",
       " 'August 2021',\n",
       " 'September 2021',\n",
       " 'June 2021',\n",
       " 'December 2016',\n",
       " 'September 2021',\n",
       " 'May 2021',\n",
       " 'January 2014',\n",
       " 'December 1997',\n",
       " 'October 2021',\n",
       " 'February 2010']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = []\n",
    "\n",
    "for i in soup.find_all('span',class_=\"sc-1thf9ly-2 bKddwo\"):\n",
    "    date.append(i.text)\n",
    "    \n",
    "    \n",
    "date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sciencedirect.com/science/article/pii/S0004370221000862',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000722',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370215000910',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370298000551',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370216300790',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370218305988',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370220301855',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370214001386',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370299000521',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370219300116',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370220301533',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370207000793',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370216300285',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370220301958',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370297000635',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000515',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000539',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000096',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370216300868',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000588',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000102',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370213001082',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S000437029700043X',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000734',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370209001398']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = []\n",
    "for i in soup.find_all(\"a\", class_=\"sc-5smygv-0 nrDZj\"):\n",
    "    link.append(i['href'])\n",
    "link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>location</th>\n",
       "      <th>ratings</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>₹ 2,000 for 2 (approx) | Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>₹ 1,680 for 2 (approx) | North Indian, Asian, ...</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>₹ 3,000 for 2 (approx) | Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>₹ 2,000 for 2 (approx) | Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>₹ 1,700 for 2 (approx) | North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>₹ 2,400 for 2 (approx) | North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>₹ 1,800 for 2 (approx) | North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>₹ 1,900 for 2 (approx) | North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>₹ 2,200 for 2 (approx) | North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             titles  \\\n",
       "0                   Castle Barbeque   \n",
       "1                   Jungle Jamboree   \n",
       "2                        Cafe Knosh   \n",
       "3                   Castle Barbeque   \n",
       "4              The Barbeque Company   \n",
       "5                       India Grill   \n",
       "6                    Delhi Barbeque   \n",
       "7  The Monarch - Bar Be Que Village   \n",
       "8                 Indian Grill Room   \n",
       "\n",
       "                                             cuisine  \\\n",
       "0     ₹ 2,000 for 2 (approx) | Chinese, North Indian   \n",
       "1  ₹ 1,680 for 2 (approx) | North Indian, Asian, ...   \n",
       "2      ₹ 3,000 for 2 (approx) | Italian, Continental   \n",
       "3     ₹ 2,000 for 2 (approx) | Chinese, North Indian   \n",
       "4     ₹ 1,700 for 2 (approx) | North Indian, Chinese   \n",
       "5     ₹ 2,400 for 2 (approx) | North Indian, Italian   \n",
       "6              ₹ 1,800 for 2 (approx) | North Indian   \n",
       "7              ₹ 1,900 for 2 (approx) | North Indian   \n",
       "8     ₹ 2,200 for 2 (approx) | North Indian, Mughlai   \n",
       "\n",
       "                                            location ratings  \\\n",
       "0                     Connaught Place, Central Delhi     4.1   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
       "2  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "3             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "4                 Gardens Galleria,Sector 38A, Noida       4   \n",
       "5               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "\n",
       "                                              images  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page = requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "soup = BeautifulSoup(page.content)\n",
    "restaurant_name = soup.find('a', class_=\"restnt-name ellipsis\")\n",
    "restaurant_name.text\n",
    "cuisine =[]\n",
    "for  i in soup.find_all('span', class_=\"double-line-ellipsis\"):\n",
    "     cuisine.append(i.text)\n",
    "cuisine\n",
    "location= soup.find('div', class_=\"restnt-loc ellipsis\")\n",
    "location.text\n",
    "ratings= soup.find('div', class_=\"restnt-rating rating-4\")\n",
    "ratings.text\n",
    "images = []\n",
    "for i in soup.find_all('img' , class_=\"no-img\"):\n",
    "    images.append(i['data-src'])\n",
    "images\n",
    "titles =[]\n",
    "for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    titles.append(i.text)\n",
    "    \n",
    "titles\n",
    "location =[] \n",
    "for i in soup.find_all('div', class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(i.text)\n",
    "location\n",
    "ratings=[] \n",
    "for i in soup.find_all('div', class_=\"restnt-rating rating-4\"):\n",
    "    ratings.append(i.text)\n",
    "ratings\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'titles':titles,'cuisine':cuisine,'location':location,'ratings':ratings,'images':images})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>publication</th>\n",
       "      <th>h5index</th>\n",
       "      <th>h5median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    rank                                        publication h5index h5median\n",
       "0     1.                                             Nature     444      667\n",
       "1     2.                The New England Journal of Medicine     432      780\n",
       "2     3.                                            Science     401      614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...     389      627\n",
       "4     5.                                         The Lancet     354      635\n",
       "..   ...                                                ...     ...      ...\n",
       "95   96.                       Journal of Business Research     145      233\n",
       "96   97.                                   Molecular Cancer     145      209\n",
       "97   98.                                            Sensors     145      201\n",
       "98   99.                              Nature Climate Change     144      228\n",
       "99  100.                    IEEE Internet of Things Journal     144      212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "page = requests.get('https://scholar.google.com/citations?view_op=top_venues&hl=en')\n",
    "soup = BeautifulSoup(page.content)\n",
    "rank =[]\n",
    "for  i in soup.find_all('td', class_=\"gsc_mvt_p\"):\n",
    "     rank.append(i.text)\n",
    "rank\n",
    "publication=[]\n",
    "for i in soup.find_all('td', class_=\"gsc_mvt_t\"):\n",
    "    publication.append(i.text)\n",
    "publication\n",
    "h5index = [] \n",
    "for i in soup.find_all('a', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5index.append(i.text)\n",
    "h5index\n",
    "h5median = []\n",
    "for i in soup.find_all('span', class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5median.append(i.text)\n",
    "h5median\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'rank':rank,'publication':publication,'h5index':h5index,'h5median':h5median})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
